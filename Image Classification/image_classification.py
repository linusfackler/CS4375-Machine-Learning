# -*- coding: utf-8 -*-
"""Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JfZ5DSdWOxvzCsrbUe-Fj4gXtAhZAmN8

<h1 align="center">Image Classification</h1>
<h2 align="center">Object Detection for chess pieces</h2>
<h4 align="center">Linus Fackler and Fernando Colman</h4>

# About the dataset

The dataset we are using is from https://public.roboflow.com/object-detection/chess-full
The reason we chose this dataset was because it has the specific labeling type needed for training a YoloV7 model.
The dataset consists of imgages of chessboard with chesspieces.
There are 13 different classes:
'bishop', 'black-bishop', 'black-king', 'black-knight', 'black-pawn', 'black-queen', 'black-rook', 'white-bishop', 'white-king', 'white-knight', 'white-pawn', 'white-queen', 'white-rook'

### What the model should be able to predict
The model should be able to recognize chess pieces and identify which type of chess piece it is (bishop, pawn, rook, ...)


### Preprocessing of Data
There is no preprocessing that needs to be done here.

### Split into train, test, val
This step is also not necessary with this dataset, as it already comes with a premade split.
There are 202 train images, 58 validation and 29 test.
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os

"""# Sequential Model

### Reading in data

I've had problems with my original chess dataset for this part of the project. That's why I am using a tensorflow example dataset, so that I can at least get some results.
"""

import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds
import pathlib

from google.colab import drive
drive.mount('/content/drive')

import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

image_count = len(list(data_dir.glob('*/*.jpg')))
print("Number of images: ", image_count)

"""Here we took 270 images out of the list and put it into test"""

roses = list(data_dir.glob('roses/*'))
PIL.Image.open(str(roses[0]))

# %cd ..
# %ls
# train_url = "/content/drive/MyDrive/Colab Notebooks/Chess dataset/train"
# test_url = "/content/drive/MyDrive/Colab Notebooks/Chess dataset/test"
# val_url = "/content/drive/MyDrive/Colab Notebooks/Chess dataset/valid"

# train_ds = tf.keras.utils.image_dataset_from_directory(train_url, labels=None, validation_split=None, image_size=(200, 134), batch_size=16)
# test_ds = tf.keras.utils.image_dataset_from_directory(test_url, labels=None, validation_split=None, image_size=(200, 134), batch_size=16)
# val_ds = tf.keras.utils.image_dataset_from_directory(val_url, labels=None, validation_split=None, image_size=(200, 134), batch_size=16)

"""### Creating a dataset"""

batch_size = 32
img_height = 180
img_width = 180

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.4,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

test_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=None,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print("Classes: ", class_names)

"""### Visualizing the Data"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

"""### Creating a model"""

num_classes = len(class_names)

model = tf.keras.models.Sequential([
  tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes)
])

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""### Training the model"""

history = model.fit(train_ds, batch_size=32, epochs=10, validation_data=val_ds)

history.history.keys()

"""### Evaluate the model"""

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""# CNN architecture

We are going to use the same dataset as we did for the sequential.
"""

num_filters = 8
filter_size = 3
pool_size = 2

model2 = tf.keras.models.Sequential(
    [
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(num_classes, activation="softmax"),
    ]
)

model2.summary()

"""### Train model"""

model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history2 = model2.fit(train_ds, batch_size=32, epochs=10, validation_data=val_ds)

"""### Evaluate the model"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

"""# Pretrained Model
I've decided to use the YOLOv7 model for this project, as it seems to be the best open-source model for objects detection.
I personally have experience with it, which was another factor as to why I chose this.

![](https://raw.githubusercontent.com/WongKinYiu/yolov7/main/figure/performance.png)
"""

# Commented out IPython magic to ensure Python compatibility.
# Downloading YOLO v7 Code
!git clone https://github.com/WongKinYiu/yolov7.git
# %cd yolov7
!ls

# Pre-trained weights
!wget https://github.com/WongKinbYiu/yolov7/releases/download/v0.1/yolov7.pt

"""### Resizing images

I decided to later comment out this section, as yolov7 does its own image resizing. Originallz I wanted to do this so that it would not train with 2048px images, which would take too long.

I still kept it for future references, as I think the code is very helpful.
Also, in case I ever want to use a different pretrained model, this might become helpful.
"""

# from PIL import Image

# directory = "/content/drive/MyDrive/Colab Notebooks/Chess/train/images"

# basewidth = 640

# for filename in os.listdir(directory):
#   f = os.path.join(directory, filename)
#   img = Image.open(f)
#   wpercent = (basewidth / float(img.size[0]))
#   hsize = int((float(img.size[1]) * float(wpercent)))
#   img = img.resize((basewidth, hsize), Image.ANTIALIAS)
#   img.save(f)

"""### Training

This is where the **transfer learning** comes into play. We will now train the pretrained model with a new type of image. The images won't consist any of the 80 previous classes from the YOLOv7 (which actually is from the coco dataset), now we will only have our 13 classes, which include the chess figures.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/yolov7"
!python train.py --epochs 30 --weights yolov7.pt --batch-size 16 --data "/content/drive/MyDrive/Colab Notebooks/Chess/data.yaml" --name yolov7-custom --device 0 --exist-ok

from PIL import Image
pcurve = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/train/yolov7-custom/P_curve.png')
confusionmatrix = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/train/yolov7-custom/confusion_matrix.png')
results = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/train/yolov7-custom/results.png')

display(pcurve)
display(confusionmatrix)
display(results)

"""### Testing"""

!python test.py --data "/content/drive/MyDrive/Colab Notebooks/Chess/data.yaml" --batch-size 16 --weights "/content/drive/MyDrive/Colab Notebooks/yolov7/runs/train/yolov7-custom/weights/best.pt" --exist-ok

pcurve = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/test/exp/P_curve.png')
confusionmatrix = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/test/exp/confusion_matrix.png')
#results = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/test/exp/results.png')

display(pcurve)
display(confusionmatrix)
#display(results)

"""The results here look terrible, *but* that was probably just because of the low number of epochs, which still took me forever to train. At first I tried it with 5 epochs, and it was even worse (I know, hard to imagine). With 300 epochs it would have probably been very good.

### Detecting

Running sample images on the model on which we used transfer learning on.
"""

!python detect.py --weights "/content/drive/MyDrive/Colab Notebooks/yolov7/runs/train/yolov7-custom/weights/best.pt" --conf 0.25 --img-size 640 --source "/content/drive/MyDrive/Colab Notebooks/Chess/test/images/a3863d0be6002c21b20ac88817b2c56f_jpg.rf.e421134b139d57e02e7df9468a35c1fb.jpg" --exist-ok

pil_im1 = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/test/exp/test_batch0_labels.jpg')
pil_im2 = Image.open('/content/drive/MyDrive/Colab Notebooks/yolov7/runs/test/exp/test_batch2_labels.jpg')
display(pil_im1)
display(pil_im2)

"""# Analysis

Since I have had problems with my originally chosen dataset, I will not be able to compare the Sequential/RNN with the pretrained yolov7 model.

Generally, I can say, after working for quite a few time with a pretrained model, such as the YOLOv7, and before that the YOLOv5, I enjoy this more than building a model from scratch/using tensorflows sequential models. Not only are pretrained models higher in accuracy (well, I couldn't prove it this time, but there's a reason as to why those are so popular), but also you will save time by just going with the (probably objectively) better choice. Especially, if you want to use features like opencv and detect objects in a real life setting with your camera or a video recording.

There is still the option to, as we have done it here, use transfer learning to use an existing model, like the YOLOv7, and "customize it" for your dataset. I have personally done this over the course of this semester, as I was part of the ACM Research. We used the FLIR thermal imaging dataset to make our own model (YOLOvCAPY), which works best for thermal images, especially when detecting objects while driving a car, which is what the dataset was made for.

For a general purpose like this here, the Sequential or RNN model seemed alright, but as soon as you want to expand, it seems impossible with these limited resources.
"""