---
title: "Regression"
author: "Linus Fackler, Fernando Colman"
output:
  word_document: default
  html_notebook: default
---

This data set contains information about car sales to predict the price of a car given a set of data.
This information contains the price, year, model, odometer reading, and other values.
The data set can be found here: https://www.kaggle.com/datasets/deepcontractor/car-price-prediction-challenge

### Load the data & package
```{r}
library(e1071)
df <- read.csv("car_price_prediction.csv", header = TRUE)
str(df)
```
### Data cleaning

First, we have to clean the data. Some columns are unnecessary, which is why we are going to throw them out.
Some contain numbers followed by characters, which is why we have to convert them first to just numbers.

We throw out columns "ID", "Levy", and "Engine volume", since they don't help us much in predicting the car price.
```{r}
df <- df[, c(2,6,7,8,9,11,12,13,14,15,16,17,18)]
str(df)
```
We check how many NA rows there are.

```{r}
sapply(df, function(x) sum(is.na(x)==TRUE))
```
None, that's great.

For simplicity reasons, we are going to rename some of the columns, as their names are too unnecessarily long.

```{r}
names(df)[2] <- "Year"
names(df)[4] <- "Leather"
names(df)[5] <- "Fuel"
names(df)[8] <- "Gearbox"
names(df)[9] <- "Drivetrain"
str(df)
```

We are going to check, how many unique values the column "Leather" seats has.
```{r}
unique(df$Leather)
```
There is no need for further changing this column.

We will need to cut off the "km" after the mileage, to make it into an integer value.

```{r}
df$Mileage[1:10]
```
```{r}
df$Mileage <- gsub(" .*","",df$Mileage)
df$Mileage[1:10]
```
Now, we transform it to integer values.

```{r}
df <- transform(df, Mileage = as.integer(Mileage))
str(df)
```

Let's check the maximum value of mileage.

```{r}
max(df$Mileage)
length(df$Mileage[df$Mileage > 500000])
```

We see that 2147483647 is a very unrealistic number of kilometers.
There are 258 observations with a mileage of over 500,000 kilometers, which is why we will throw them out.

```{r}
df <- df[df$Mileage < 500000,]
```

We can check the same for the price.

```{r}
max(df$Price)
length(df$Price[df$Price > 200000])
```
These unrealistically high values will negatively impact our data, which is why we will throw everything over 200,000 out.

```{r}
df <- df[df$Price < 200000,]
```


We will also change Cylinders to integer values.

```{r}
df <- transform(df, Cylinders = as.integer(Cylinders))
str(df)
```

For some reason, the doors have "May" and "Mar" attached to them. We will get rid of them and change it to integer values.

```{r}
unique(df$Doors)
```

```{r}
length(df$Doors[df$Doors == ">5"])
df$Model[df$Doors == ">5"][1:20]
```

After looking closer into rows where doors are ">5", we see that this is most likely just an error, so we will throw these rows out.
It is just 128 out of over 19,000 rows, so it won't affect our results.

```{r}
df <- df[!(df$Doors == ">5"),]
```

We are now going to take out the -Mar and -May, and convert it to integer values

```{r}
df$Doors <- gsub("-.*","",df$Doors)
df$Doors <- gsub("0","",df$Doors)
df$Doors[1:10]

df <- transform(df, Doors = as.integer(Doors))
str(df)
```

Factorizing all chr data

```{r}
df$Category <- factor(df$Category)
df$Leather <- factor(df$Leather)
df$Fuel <- factor(df$Fuel)
df$Gearbox <- factor(df$Gearbox)
df$Drivetrain <- factor(df$Drivetrain)
df$Wheel <- factor(df$Wheel)
df$Color <- factor(df$Color)

str(df)
```



### Train and Test sets

Divide into train and test sets

```{r}
set.seed(1234)
spec <- c(train=.6, test=.2, validate=.2)
i <- sample(cut(1:nrow(df), nrow(df)*cumsum(c(0,spec)), labels=names(spec)))
train <- df[i=="train",]
test <- df[i=="test",]
vald <- df[i=="validate",]
```

### Data Exploration of Training Data

First, we will explore the training data statistically and graphically

```{r}
print(paste("Number of Rows: ", nrow(train)))

print(paste("Average price: ", mean(train$Price)))
print(paste("Median price: ", median(train$Price)))

print(paste("Average mileage: ", mean(train$Mileage)))
print(paste("Median mileage: ", median(train$Mileage)))

print(paste("Avergae number of airbags: ", mean(train$Airbags)))
print(paste("Median number of airbags: ", median(train$Airbags)))

```

This seems like pretty realist data for car sales.

Now, we'll plot the car price and mileage, to see if there is some obvious correlation.

```{r}
plot(train$Price ~ train$Mileage, xlab = "Car Mileage", ylab = "Car Price", yaxt = "n", xaxt="n", col = 3, pch = 19)
xTicks = axTicks(1)
yTicks = axTicks(2)
axis(1, at=xTicks, labels = paste(formatC(xTicks / 1000, format = 'd'), 'k', sep = ' '))
axis(2, at=yTicks, labels = paste(formatC(yTicks / 1000, format = 'd'), 'k', sep = ' '))
abline(lm(train$Price ~ train$Mileage), col = 2)
```

As expected, the price goes down if the mileage goes up.
Let's take a closer look at that.

We will group the cars into their production years.

```{r}
colors <- c("#2EFF00", #green:  before 1980
            "#FF0000", #red:    between 1980 - 1999
            "#FFFB00", #yellow  between 2000 - 2014
            "#000CFF" #blue     from 2015
           )

yrs <- train$Year
group <- ifelse(yrs < 1980, 1, ifelse(yrs < 2000, 2, ifelse(yrs < 2015, 3, 4)))


plot(train$Price ~ train$Mileage, xlab = "Car Mileage", ylab = "Car Price", yaxt = "n", xaxt="n",  col=colors[group], pch = 19, ylim=c(0,50000))
xTicks = axTicks(1)
yTicks = axTicks(2)
axis(1, at=xTicks, labels = paste(formatC(xTicks / 1000, format = 'd'), 'k', sep = ' '))
axis(2, at=yTicks, labels = paste(formatC(yTicks / 1000, format = 'd'), 'k', sep = ' '))
abline(lm(train$Price ~ train$Mileage), col = 1)

legend("topright", legend=c("Before 1980", "1980 - 1999", "2000 - 2014", "from 2015"), pch = 19, col=colors)
```

This shows us that cars with a production year after 2014 will tend to be more expensive and have less kilometers.
Cars that are over 20 years old are in the lower third of the prices.
Cars older than 42 are not seen at all in this graph.

```{r}
length(train$Year[train$Year < 1980])
```

No surprise, there is only 20 cars in our training data set that is built before 1980.


Now, let's look at cars with a value of 50 - 200k.

```{r}
colors <- c("#2EFF00", #green:  before 1980
            "#FF0000", #red:    between 1980 - 1999
            "#FFFB00", #yellow  between 2000 - 2014
            "#000CFF" #blue     from 2015
           )

yrs <- train$Year
group <- ifelse(yrs < 1980, 1, ifelse(yrs < 2000, 2, ifelse(yrs < 2015, 3, 4)))


plot(train$Price ~ train$Mileage, xlab = "Car Mileage", ylab = "Car Price", yaxt = "n", xaxt="n",  col=colors[group], pch = 19, ylim=c(50000,200000))
xTicks = axTicks(1)
yTicks = axTicks(2)
axis(1, at=xTicks, labels = paste(formatC(xTicks / 1000, format = 'd'), 'k', sep = ' '))
axis(2, at=yTicks, labels = paste(formatC(yTicks / 1000, format = 'd'), 'k', sep = ' '))

legend("topright", legend=c("Before 1980", "1980 - 1999", "2000 - 2014", "from 2015"), pch = 19, col=colors)
```

This section is largely dominated by cars built after 2014.



```{r}
colors <- c("#2EFF00", #green:  before 1980
            "#FF0000", #red:    between 1980 - 1999
            "#FFFB00", #yellow  between 2000 - 2014
            "#000CFF" #blue     from 2015
           )

yrs <- train$Year
group <- ifelse(yrs < 1980, 1, ifelse(yrs < 2000, 2, ifelse(yrs < 2015, 3, 4)))


plot(train$Price ~ train$Mileage, xlab = "Car Mileage", ylab = "Car Price", yaxt = "n", xaxt="n",  col=colors[group], pch = 19, ylim=c(0,20000))
xTicks = axTicks(1)
yTicks = axTicks(2)
axis(1, at=xTicks, labels = paste(formatC(xTicks / 1000, format = 'd'), 'k', sep = ' '))
axis(2, at=yTicks, labels = paste(formatC(yTicks / 1000, format = 'd'), 'k', sep = ' '))
abline(lm(train$Price ~ train$Mileage), col = 1)
legend("topright", legend=c("Before 1980", "1980 - 1999", "2000 - 2014", "from 2015"), pch = 19, col=colors)
```

This plot shows cars with a price under 20,000.
Again, the price goes down the more kilometers it has.

```{r}
counts <- table(train$Category)
barplot(counts, ylab = "Number of cars", xlab = "", xaxt = "n", col = 4)

axis(1, labels=FALSE)
text(x = 0:(length(counts) - 1),
     y = -1500,
     labels = paste("     ", names(counts)),
     xpd = NA,
     srt = 35,
     cex = 1.1,
     adj = 0)
```



## Linear Regression

```{r}
lm1 <- lm(Price~., data=train)
summary(lm1)
pred <- predict(lm1, newdata = test)
cor_lm1 <- cor(pred, test$Price)
mse_lm1 <- mean((pred - test$Price) ^2)

print(paste("cor=", cor_lm1))
print(paste("mse=", mse_lm1))
```

## Linear Kernel

We will have to use smaller data samples, otherwise my computer won't be able to compute the following models.
SVM with 11000 observations takes about 8 minutes. Tuning couldn't even finish, it reached the maximum number of iterations.

```{r}
trainsmall <- head(train, 2000)
testsmall <- head(test, 500)
valdsmall <- head(vald, 500)
svm1 <- svm(Price~., data=trainsmall, kernel="linear", cost=10, scale=TRUE)
summary(svm1)
```

```{r}
pred <- predict(svm1, newdata=testsmall)
cor_svm1 <- cor(pred, testsmall$Price)
mse_svm1 <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm1))
print(paste("mse=", mse_svm1))
```


### Tune

```{r}
tune_svm1 <- tune(svm, Price~. , data=valdsmall, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
```

### Evaluate on best linear svm

```{r}
pred <- predict(tune_svm1$best.model, newdata = testsmall)
cor_svm1_tune <- cor(pred, testsmall$Price)
mse_svm1_tune <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm1_tune))
print(paste("mse=", mse_svm1_tune))
```


## Polynomial Kernel

```{r}
svm2 <- svm(Price~., data=trainsmall, kernel="polynomial", cost=10, scale = TRUE)
summary(svm2)
```
 
```{r}
pred <- predict(svm2, newdata = testsmall)
cor_svm2 <- cor(pred, testsmall$Price)
mse_svm2 <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm2))
print(paste("mse=", mse_svm2))
```

### Tune

```{r}
tune_svm2 <- tune(svm, Price~. , data=valdsmall, kernel="polynomial", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm2)
```

### Evaluate on best linear svm

```{r}
pred <- predict(tune_svm2$best.model, newdata = testsmall)
cor_svm2_tune <- cor(pred, testsmall$Price)
mse_svm2_tune <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm2_tune))
print(paste("mse=", mse_svm2_tune))
```

The pre-tuned polynomial svm was better, as this used a 100 cost.


## Radial Kernel

```{r}
svm3 <- svm(Price~., data=trainsmall, kernel="radial", cost=10, scale=TRUE)
summary(svm3)
```


```{r}
pred <- predict(svm3, newdata = testsmall)
cor_svm3 <- cor(pred, testsmall$Price)
mse_svm3 <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm3))
print(paste("mse=", mse_svm3))
```


### Tune hyperperameters

```{r}
set.seed(1234)
tune.out <- tune(svm, Price~., data=valdsmall, kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
```

Cost = 10 and gamma = 0.5 shows clearly the lowest error and dispersion.

```{r}
svm4 <- svm(Price~., data=trainsmall, kernel="radial", cost=10, gamma=0.5, scale=TRUE)
summary(svm4)
```

```{r}
pred <- predict(svm4, newdata = testsmall)
cor_svm4 <- cor(pred, testsmall$Price)
mse_svm4 <- mean((pred - testsmall$Price) ^2)
print(paste("cor=", cor_svm4))
print(paste("mse=", mse_svm4))
```

## Analysis

The Radial Kernel with tuned hyperparameters will give us the best result of 0.72 correlation.
The second best was the polynomial kernel with 0.63, and third was the linear kernel with 0.52.
Linear Regression was as good as the linear kernel SVM.

Looking at the data provided, it was obvious that the radial kernel would outperform the other ones.
The data is very cluttered and is not at all linearly separable.
This is why the linear kernel didn't work well.
The polynomial was definitely better, but still couldn't perfectly handle our messy data.

Therefore, the radial was best in this case.
With a very big difference to the linear regression.